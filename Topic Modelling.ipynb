{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition, ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. LOADING TEXT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('corpus', encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "for line in data.split('\\n'):\n",
    "    label = line.split()[0]\n",
    "    all_labels.append(label)\n",
    "dist_labels = list(set(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__label__2', '__label__1']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, text = [],[]\n",
    "for i, line in enumerate(data.split('\\n')):\n",
    "    label.append(line.split()[0])\n",
    "    text.append(' '.join(line.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['__label__2',\n",
       "  '__label__2',\n",
       "  '__label__2',\n",
       "  '__label__2',\n",
       "  '__label__2',\n",
       "  '__label__2',\n",
       "  '__label__1',\n",
       "  '__label__2',\n",
       "  '__label__2',\n",
       "  '__label__2'],\n",
       " ['Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^',\n",
       "  \"The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\",\n",
       "  'Amazing!: This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you\\'ve played the game) and the hope in \"A Distant Promise\" and \"Girl who Stole the Star\" have been an important inspiration to me personally throughout my teen years. The higher energy tracks like \"Chrono Cross ~ Time\\'s Scar~\", \"Time of the Dreamwatch\", and \"Chronomantique\" (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer\\'s work (I haven\\'t heard the Xenogears soundtrack, so I can\\'t say for sure), and even if you\\'ve never played the game, it would be worth twice the price to buy it.I wish I could give it 6 stars.'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[:10], text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['labels'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['texts'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       labels                                              texts\n",
       "0  __label__2  Stuning even for the non-gamer: This sound tra...\n",
       "1  __label__2  The best soundtrack ever to anything.: I'm rea...\n",
       "2  __label__2  Amazing!: This soundtrack is my favorite music...\n",
       "3  __label__2  Excellent Soundtrack: I truly like this soundt...\n",
       "4  __label__2  Remember, Pull Your Jaw Off The Floor After He..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. PREPARING TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['texts'], trainDF['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encode the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_valid = encoder.fit_transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['__label__1', '__label__2'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1. count vectorizer from sklearn feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html import unescape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_preprocessor(doc):\n",
    "    return(unescape(doc).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,2), stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_count = count_vect.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid_count = count_vect.transform(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wm2df(wmx, feat_names):\n",
    "    \n",
    "    # create an index for each row\n",
    "    doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wmx)]\n",
    "    df = pd.DataFrame(data=wmx.toarray(), index=doc_names,\n",
    "                      columns=feat_names)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0 05</th>\n",
       "      <th>0 10</th>\n",
       "      <th>0 20gb</th>\n",
       "      <th>0 3</th>\n",
       "      <th>0 394</th>\n",
       "      <th>0 4</th>\n",
       "      <th>0 5</th>\n",
       "      <th>0 5ml</th>\n",
       "      <th>0 7</th>\n",
       "      <th>...</th>\n",
       "      <th>étai fidèle</th>\n",
       "      <th>était</th>\n",
       "      <th>était pas</th>\n",
       "      <th>étre</th>\n",
       "      <th>étre publié</th>\n",
       "      <th>éviter</th>\n",
       "      <th>última</th>\n",
       "      <th>última parte</th>\n",
       "      <th>única</th>\n",
       "      <th>única opción</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 241330 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  0 05  0 10  0 20gb  0 3  0 394  0 4  0 5  0 5ml  0 7  ...  \\\n",
       "Doc0  0     0     0       0    0      0    0    0      0    0  ...   \n",
       "Doc1  0     0     0       0    0      0    0    0      0    0  ...   \n",
       "Doc2  0     0     0       0    0      0    0    0      0    0  ...   \n",
       "Doc3  0     0     0       0    0      0    0    0      0    0  ...   \n",
       "Doc4  0     0     0       0    0      0    0    0      0    0  ...   \n",
       "\n",
       "      étai fidèle  était  était pas  étre  étre publié  éviter  última  \\\n",
       "Doc0            0      0          0     0            0       0       0   \n",
       "Doc1            0      0          0     0            0       0       0   \n",
       "Doc2            0      0          0     0            0       0       0   \n",
       "Doc3            0      0          0     0            0       0       0   \n",
       "Doc4            0      0          0     0            0       0       0   \n",
       "\n",
       "      última parte  única  única opción  \n",
       "Doc0             0      0             0  \n",
       "Doc1             0      0             0  \n",
       "Doc2             0      0             0  \n",
       "Doc3             0      0             0  \n",
       "Doc4             0      0             0  \n",
       "\n",
       "[5 rows x 241330 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wm2df(x_train_count, features).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF vectors from sklearn feature_extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern = r'\\w{1,}', max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=5000,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "                use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf = tfidf_vect.transform(x_train)\n",
    "x_valid_tfidf = tfidf_vect.transform(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=5000,\n",
       "                min_df=1, ngram_range=(2, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "                use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect_ngram.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf_ngram = tfidf_vect_ngram.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid_tfidf_ngram = tfidf_vect_ngram.transform(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_char = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=5000,\n",
       "                min_df=1, ngram_range=(2, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "                use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect_char.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf_char = tfidf_vect_char.transform(x_train)\n",
    "x_valid_tfidf_char = tfidf_vect_char.transform(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORD EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "token.fit_on_texts(trainDF['texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'i': 3,\n",
       " 'a': 4,\n",
       " 'to': 5,\n",
       " 'of': 6,\n",
       " 'it': 7,\n",
       " 'this': 8,\n",
       " 'is': 9,\n",
       " 'in': 10,\n",
       " 'for': 11,\n",
       " 'that': 12,\n",
       " 'was': 13,\n",
       " 'book': 14,\n",
       " 'you': 15,\n",
       " 'not': 16,\n",
       " 'but': 17,\n",
       " 'with': 18,\n",
       " 'on': 19,\n",
       " 'my': 20,\n",
       " 'have': 21,\n",
       " 'as': 22,\n",
       " 'are': 23,\n",
       " 'one': 24,\n",
       " 'be': 25,\n",
       " 'so': 26,\n",
       " 'all': 27,\n",
       " 'if': 28,\n",
       " 'very': 29,\n",
       " 'like': 30,\n",
       " 'read': 31,\n",
       " 'good': 32,\n",
       " 'great': 33,\n",
       " 'at': 34,\n",
       " 'movie': 35,\n",
       " 'they': 36,\n",
       " 'just': 37,\n",
       " 'about': 38,\n",
       " 'from': 39,\n",
       " 'or': 40,\n",
       " 'would': 41,\n",
       " 'an': 42,\n",
       " 'me': 43,\n",
       " 'out': 44,\n",
       " 'what': 45,\n",
       " 'has': 46,\n",
       " 'more': 47,\n",
       " 'by': 48,\n",
       " 'time': 49,\n",
       " 'had': 50,\n",
       " 'when': 51,\n",
       " 'get': 52,\n",
       " 'will': 53,\n",
       " \"it's\": 54,\n",
       " 'up': 55,\n",
       " 'there': 56,\n",
       " 'no': 57,\n",
       " 'only': 58,\n",
       " 'your': 59,\n",
       " 'can': 60,\n",
       " \"don't\": 61,\n",
       " 'his': 62,\n",
       " 'really': 63,\n",
       " 'who': 64,\n",
       " 'some': 65,\n",
       " 'he': 66,\n",
       " 'well': 67,\n",
       " 'first': 68,\n",
       " 'her': 69,\n",
       " 'much': 70,\n",
       " 'than': 71,\n",
       " 'even': 72,\n",
       " 'do': 73,\n",
       " 'story': 74,\n",
       " 'because': 75,\n",
       " 'them': 76,\n",
       " 'other': 77,\n",
       " 'after': 78,\n",
       " 'buy': 79,\n",
       " 'we': 80,\n",
       " 'were': 81,\n",
       " 'too': 82,\n",
       " 'which': 83,\n",
       " 'she': 84,\n",
       " 'how': 85,\n",
       " 'love': 86,\n",
       " 'these': 87,\n",
       " 'been': 88,\n",
       " 'better': 89,\n",
       " 'best': 90,\n",
       " 'could': 91,\n",
       " 'any': 92,\n",
       " 'into': 93,\n",
       " 'their': 94,\n",
       " 'did': 95,\n",
       " 'books': 96,\n",
       " 'am': 97,\n",
       " 'also': 98,\n",
       " 'work': 99,\n",
       " 'product': 100,\n",
       " 'think': 101,\n",
       " 'then': 102,\n",
       " 'way': 103,\n",
       " \"i'm\": 104,\n",
       " 'its': 105,\n",
       " 'most': 106,\n",
       " 'ever': 107,\n",
       " 'make': 108,\n",
       " 'little': 109,\n",
       " 'many': 110,\n",
       " 'bad': 111,\n",
       " 'over': 112,\n",
       " 'see': 113,\n",
       " 'cd': 114,\n",
       " 'money': 115,\n",
       " 'now': 116,\n",
       " 'never': 117,\n",
       " 'new': 118,\n",
       " 'people': 119,\n",
       " 'does': 120,\n",
       " 'back': 121,\n",
       " 'film': 122,\n",
       " 'music': 123,\n",
       " 'reading': 124,\n",
       " 'know': 125,\n",
       " 'should': 126,\n",
       " 'bought': 127,\n",
       " 'got': 128,\n",
       " 'use': 129,\n",
       " '2': 130,\n",
       " 'made': 131,\n",
       " 'want': 132,\n",
       " 'still': 133,\n",
       " 'off': 134,\n",
       " 'find': 135,\n",
       " 'recommend': 136,\n",
       " \"didn't\": 137,\n",
       " 'two': 138,\n",
       " 'album': 139,\n",
       " 'life': 140,\n",
       " 'game': 141,\n",
       " 'years': 142,\n",
       " 'dvd': 143,\n",
       " 'found': 144,\n",
       " 'say': 145,\n",
       " 'go': 146,\n",
       " \"i've\": 147,\n",
       " 'old': 148,\n",
       " 'again': 149,\n",
       " 'thought': 150,\n",
       " 'through': 151,\n",
       " 'same': 152,\n",
       " 'every': 153,\n",
       " 'while': 154,\n",
       " 'quality': 155,\n",
       " 'thing': 156,\n",
       " 'another': 157,\n",
       " \"can't\": 158,\n",
       " 'characters': 159,\n",
       " 'before': 160,\n",
       " 'down': 161,\n",
       " 'worth': 162,\n",
       " 'put': 163,\n",
       " 'something': 164,\n",
       " 'must': 165,\n",
       " 'why': 166,\n",
       " \"doesn't\": 167,\n",
       " 'written': 168,\n",
       " 'few': 169,\n",
       " 'long': 170,\n",
       " 'being': 171,\n",
       " 'version': 172,\n",
       " 'hard': 173,\n",
       " 'those': 174,\n",
       " 'give': 175,\n",
       " 'our': 176,\n",
       " 'where': 177,\n",
       " '1': 178,\n",
       " 'lot': 179,\n",
       " 'used': 180,\n",
       " 'going': 181,\n",
       " 'anyone': 182,\n",
       " 'makes': 183,\n",
       " 'waste': 184,\n",
       " 'nothing': 185,\n",
       " 'however': 186,\n",
       " '3': 187,\n",
       " 'world': 188,\n",
       " 'watch': 189,\n",
       " 'amazon': 190,\n",
       " 'looking': 191,\n",
       " 'far': 192,\n",
       " 'times': 193,\n",
       " 'here': 194,\n",
       " 'such': 195,\n",
       " 'fun': 196,\n",
       " 'look': 197,\n",
       " '5': 198,\n",
       " 'excellent': 199,\n",
       " 'need': 200,\n",
       " 'boring': 201,\n",
       " 'plot': 202,\n",
       " 'series': 203,\n",
       " 'year': 204,\n",
       " 'enough': 205,\n",
       " 'real': 206,\n",
       " 'easy': 207,\n",
       " 'end': 208,\n",
       " 'sound': 209,\n",
       " 'big': 210,\n",
       " 'interesting': 211,\n",
       " 'novel': 212,\n",
       " 'movies': 213,\n",
       " 'songs': 214,\n",
       " 'without': 215,\n",
       " 'price': 216,\n",
       " 'though': 217,\n",
       " 'own': 218,\n",
       " 'feel': 219,\n",
       " 'take': 220,\n",
       " 'day': 221,\n",
       " 'video': 222,\n",
       " 'disappointed': 223,\n",
       " 'since': 224,\n",
       " 'works': 225,\n",
       " 'original': 226,\n",
       " 'different': 227,\n",
       " 'right': 228,\n",
       " 'set': 229,\n",
       " 'last': 230,\n",
       " 'author': 231,\n",
       " 'things': 232,\n",
       " 'worst': 233,\n",
       " 'actually': 234,\n",
       " 'may': 235,\n",
       " 'around': 236,\n",
       " 'fan': 237,\n",
       " 'pretty': 238,\n",
       " 'classic': 239,\n",
       " 'nice': 240,\n",
       " '4': 241,\n",
       " 'him': 242,\n",
       " 'reviews': 243,\n",
       " 'sure': 244,\n",
       " 'us': 245,\n",
       " 'character': 246,\n",
       " 'stars': 247,\n",
       " 'seen': 248,\n",
       " 'part': 249,\n",
       " 'play': 250,\n",
       " 'come': 251,\n",
       " 'both': 252,\n",
       " 'away': 253,\n",
       " 'keep': 254,\n",
       " 'always': 255,\n",
       " 'said': 256,\n",
       " 'song': 257,\n",
       " 'high': 258,\n",
       " 'wonderful': 259,\n",
       " 'review': 260,\n",
       " 'enjoy': 261,\n",
       " 'seems': 262,\n",
       " 'each': 263,\n",
       " 'bit': 264,\n",
       " 'understand': 265,\n",
       " 'second': 266,\n",
       " 'try': 267,\n",
       " 'item': 268,\n",
       " 'problem': 269,\n",
       " 'writing': 270,\n",
       " 'information': 271,\n",
       " 'yet': 272,\n",
       " 'purchased': 273,\n",
       " 'maybe': 274,\n",
       " 'came': 275,\n",
       " 'anything': 276,\n",
       " 'believe': 277,\n",
       " 'loved': 278,\n",
       " 'almost': 279,\n",
       " 'fact': 280,\n",
       " 'show': 281,\n",
       " 'poor': 282,\n",
       " 'everything': 283,\n",
       " 'small': 284,\n",
       " 'once': 285,\n",
       " 'man': 286,\n",
       " 'quite': 287,\n",
       " 'tried': 288,\n",
       " 'action': 289,\n",
       " 'highly': 290,\n",
       " 'instead': 291,\n",
       " 'whole': 292,\n",
       " 'next': 293,\n",
       " 'less': 294,\n",
       " 'point': 295,\n",
       " 'special': 296,\n",
       " 'three': 297,\n",
       " \"couldn't\": 298,\n",
       " 'family': 299,\n",
       " 'getting': 300,\n",
       " 'using': 301,\n",
       " 'probably': 302,\n",
       " 'trying': 303,\n",
       " 'least': 304,\n",
       " 'true': 305,\n",
       " '10': 306,\n",
       " 'help': 307,\n",
       " 'done': 308,\n",
       " 'star': 309,\n",
       " 'gets': 310,\n",
       " 'full': 311,\n",
       " 'watching': 312,\n",
       " 'having': 313,\n",
       " 'able': 314,\n",
       " 'favorite': 315,\n",
       " 'fine': 316,\n",
       " 'size': 317,\n",
       " 'style': 318,\n",
       " 'live': 319,\n",
       " \"isn't\": 320,\n",
       " 'enjoyed': 321,\n",
       " 'ago': 322,\n",
       " 'especially': 323,\n",
       " 'job': 324,\n",
       " 'minutes': 325,\n",
       " 'amazing': 326,\n",
       " 'together': 327,\n",
       " 'wrong': 328,\n",
       " 'stories': 329,\n",
       " 'definitely': 330,\n",
       " 'went': 331,\n",
       " \"you're\": 332,\n",
       " 'let': 333,\n",
       " \"that's\": 334,\n",
       " 'might': 335,\n",
       " 'school': 336,\n",
       " 'pages': 337,\n",
       " 'perfect': 338,\n",
       " \"wasn't\": 339,\n",
       " 'ordered': 340,\n",
       " 'someone': 341,\n",
       " 'top': 342,\n",
       " 'toy': 343,\n",
       " 'beautiful': 344,\n",
       " 'although': 345,\n",
       " 'short': 346,\n",
       " 'cover': 347,\n",
       " 'liked': 348,\n",
       " 'terrible': 349,\n",
       " \"won't\": 350,\n",
       " 'else': 351,\n",
       " 'fit': 352,\n",
       " 'horrible': 353,\n",
       " 'piece': 354,\n",
       " 'place': 355,\n",
       " 'please': 356,\n",
       " 'buying': 357,\n",
       " 'several': 358,\n",
       " 'wish': 359,\n",
       " 'rather': 360,\n",
       " 'card': 361,\n",
       " 'kind': 362,\n",
       " 'wanted': 363,\n",
       " 'half': 364,\n",
       " 'heard': 365,\n",
       " 'already': 366,\n",
       " 'listen': 367,\n",
       " 'reason': 368,\n",
       " 'received': 369,\n",
       " 'until': 370,\n",
       " 'comes': 371,\n",
       " 'acting': 372,\n",
       " 'history': 373,\n",
       " 'months': 374,\n",
       " 'ok': 375,\n",
       " 'everyone': 376,\n",
       " 'mind': 377,\n",
       " 'tell': 378,\n",
       " 'class': 379,\n",
       " 'page': 380,\n",
       " 'computer': 381,\n",
       " 'between': 382,\n",
       " 'others': 383,\n",
       " 'days': 384,\n",
       " 'took': 385,\n",
       " 'purchase': 386,\n",
       " 'picture': 387,\n",
       " 'power': 388,\n",
       " 'reader': 389,\n",
       " 'gave': 390,\n",
       " 'christmas': 391,\n",
       " 'camera': 392,\n",
       " 'line': 393,\n",
       " 'order': 394,\n",
       " 'house': 395,\n",
       " 'start': 396,\n",
       " 'kids': 397,\n",
       " 'bed': 398,\n",
       " 'completely': 399,\n",
       " 'takes': 400,\n",
       " 'funny': 401,\n",
       " 'making': 402,\n",
       " 'simply': 403,\n",
       " 'saw': 404,\n",
       " 'night': 405,\n",
       " 'fast': 406,\n",
       " 'effects': 407,\n",
       " 'box': 408,\n",
       " 'either': 409,\n",
       " 'children': 410,\n",
       " 'difficult': 411,\n",
       " 'return': 412,\n",
       " 'happy': 413,\n",
       " 'collection': 414,\n",
       " 'idea': 415,\n",
       " 'light': 416,\n",
       " 'looks': 417,\n",
       " 'hours': 418,\n",
       " 'english': 419,\n",
       " 'overall': 420,\n",
       " 'cheap': 421,\n",
       " \"i'd\": 422,\n",
       " 'seem': 423,\n",
       " 'young': 424,\n",
       " 'needed': 425,\n",
       " 'myself': 426,\n",
       " 'son': 427,\n",
       " 'cannot': 428,\n",
       " 'future': 429,\n",
       " 'player': 430,\n",
       " \"you'll\": 431,\n",
       " 'couple': 432,\n",
       " 'air': 433,\n",
       " 'words': 434,\n",
       " 'felt': 435,\n",
       " 'absolutely': 436,\n",
       " 'save': 437,\n",
       " 'battery': 438,\n",
       " 'rest': 439,\n",
       " 'problems': 440,\n",
       " 'edition': 441,\n",
       " 'person': 442,\n",
       " 'home': 443,\n",
       " 'left': 444,\n",
       " 'started': 445,\n",
       " 'band': 446,\n",
       " 'lost': 447,\n",
       " 'worked': 448,\n",
       " 'truly': 449,\n",
       " 'wait': 450,\n",
       " 'ray': 451,\n",
       " 'side': 452,\n",
       " 'scenes': 453,\n",
       " 'working': 454,\n",
       " 'etc': 455,\n",
       " 'thinking': 456,\n",
       " 'simple': 457,\n",
       " 'women': 458,\n",
       " 'goes': 459,\n",
       " 'sense': 460,\n",
       " 'stuff': 461,\n",
       " 'stay': 462,\n",
       " 'fiction': 463,\n",
       " 'woman': 464,\n",
       " 'itself': 465,\n",
       " 'free': 466,\n",
       " 'title': 467,\n",
       " \"wouldn't\": 468,\n",
       " 'experience': 469,\n",
       " 'guess': 470,\n",
       " 'hear': 471,\n",
       " 'entire': 472,\n",
       " 'awesome': 473,\n",
       " 'track': 474,\n",
       " 'friends': 475,\n",
       " 'missing': 476,\n",
       " 'playing': 477,\n",
       " 'american': 478,\n",
       " 'course': 479,\n",
       " 'ending': 480,\n",
       " 'cool': 481,\n",
       " 'company': 482,\n",
       " 'case': 483,\n",
       " 'gives': 484,\n",
       " 'daughter': 485,\n",
       " 'under': 486,\n",
       " 'slow': 487,\n",
       " 'shows': 488,\n",
       " 'hope': 489,\n",
       " 'support': 490,\n",
       " 'extremely': 491,\n",
       " 'copy': 492,\n",
       " 'dont': 493,\n",
       " 'type': 494,\n",
       " 'based': 495,\n",
       " \"i'll\": 496,\n",
       " 'parts': 497,\n",
       " 'language': 498,\n",
       " 'horror': 499,\n",
       " 'complete': 500,\n",
       " 'main': 501,\n",
       " 'pictures': 502,\n",
       " 'yourself': 503,\n",
       " 'finally': 504,\n",
       " 'expected': 505,\n",
       " 'girl': 506,\n",
       " 'glad': 507,\n",
       " '6': 508,\n",
       " 'played': 509,\n",
       " 'past': 510,\n",
       " 'learn': 511,\n",
       " 'tracks': 512,\n",
       " 'self': 513,\n",
       " 'unfortunately': 514,\n",
       " 'along': 515,\n",
       " 'hold': 516,\n",
       " 'run': 517,\n",
       " 'comfortable': 518,\n",
       " 'ones': 519,\n",
       " 'care': 520,\n",
       " 'garbage': 521,\n",
       " 'says': 522,\n",
       " 'needs': 523,\n",
       " 'dark': 524,\n",
       " 'sounds': 525,\n",
       " 'name': 526,\n",
       " 'scary': 527,\n",
       " 'yes': 528,\n",
       " 'today': 529,\n",
       " 'unless': 530,\n",
       " 'easily': 531,\n",
       " 'during': 532,\n",
       " 'release': 533,\n",
       " 'totally': 534,\n",
       " 'huge': 535,\n",
       " 'entertaining': 536,\n",
       " 'available': 537,\n",
       " 'later': 538,\n",
       " 'doing': 539,\n",
       " 'view': 540,\n",
       " 'mr': 541,\n",
       " 'called': 542,\n",
       " '1984': 543,\n",
       " 'kept': 544,\n",
       " 'tv': 545,\n",
       " 'pay': 546,\n",
       " 'loves': 547,\n",
       " 'society': 548,\n",
       " 'remember': 549,\n",
       " 'black': 550,\n",
       " 'guy': 551,\n",
       " 'told': 552,\n",
       " 'gift': 553,\n",
       " \"there's\": 554,\n",
       " 'single': 555,\n",
       " 'heart': 556,\n",
       " 'given': 557,\n",
       " 'seemed': 558,\n",
       " 'five': 559,\n",
       " 'sometimes': 560,\n",
       " 'recommended': 561,\n",
       " 'war': 562,\n",
       " 'service': 563,\n",
       " 'write': 564,\n",
       " 'friend': 565,\n",
       " 'store': 566,\n",
       " 'turn': 567,\n",
       " 'opinion': 568,\n",
       " 'child': 569,\n",
       " 'color': 570,\n",
       " 'word': 571,\n",
       " 'expect': 572,\n",
       " 'sorry': 573,\n",
       " 'early': 574,\n",
       " 'disappointing': 575,\n",
       " 'within': 576,\n",
       " 'weeks': 577,\n",
       " 'description': 578,\n",
       " 'interested': 579,\n",
       " 'become': 580,\n",
       " 'deal': 581,\n",
       " 'text': 582,\n",
       " 'stop': 583,\n",
       " 'fans': 584,\n",
       " 'cut': 585,\n",
       " 'clear': 586,\n",
       " 'age': 587,\n",
       " 'decided': 588,\n",
       " 'often': 589,\n",
       " 'rock': 590,\n",
       " 'mean': 591,\n",
       " 'looked': 592,\n",
       " 'stupid': 593,\n",
       " 'head': 594,\n",
       " 'worse': 595,\n",
       " 'kindle': 596,\n",
       " 'beginning': 597,\n",
       " 'awful': 598,\n",
       " 'lots': 599,\n",
       " 'low': 600,\n",
       " 'paper': 601,\n",
       " 'perhaps': 602,\n",
       " 'follow': 603,\n",
       " 'human': 604,\n",
       " 'week': 605,\n",
       " 'print': 606,\n",
       " 'change': 607,\n",
       " 'important': 608,\n",
       " 'screen': 609,\n",
       " 'value': 610,\n",
       " 'science': 611,\n",
       " 'non': 612,\n",
       " 'level': 613,\n",
       " 'number': 614,\n",
       " 'ideas': 615,\n",
       " 'fantastic': 616,\n",
       " 'art': 617,\n",
       " 'games': 618,\n",
       " 'god': 619,\n",
       " 'wear': 620,\n",
       " 'modern': 621,\n",
       " 'john': 622,\n",
       " 'watched': 623,\n",
       " 'close': 624,\n",
       " 'strong': 625,\n",
       " 'supposed': 626,\n",
       " 'chapter': 627,\n",
       " 'arrived': 628,\n",
       " 'husband': 629,\n",
       " 'wrote': 630,\n",
       " 'dance': 631,\n",
       " 'listening': 632,\n",
       " 'charger': 633,\n",
       " 'voice': 634,\n",
       " 'u': 635,\n",
       " 'films': 636,\n",
       " 'tale': 637,\n",
       " \"haven't\": 638,\n",
       " 'add': 639,\n",
       " 'four': 640,\n",
       " 'month': 641,\n",
       " 'sad': 642,\n",
       " 'usually': 643,\n",
       " 'replacement': 644,\n",
       " 'hot': 645,\n",
       " 'ear': 646,\n",
       " 'printer': 647,\n",
       " 'quickly': 648,\n",
       " 'disc': 649,\n",
       " 'scene': 650,\n",
       " 'literature': 651,\n",
       " 'taking': 652,\n",
       " 'check': 653,\n",
       " '8': 654,\n",
       " 'cost': 655,\n",
       " 'material': 656,\n",
       " 's': 657,\n",
       " 'poorly': 658,\n",
       " 'disappointment': 659,\n",
       " 'oh': 660,\n",
       " 'large': 661,\n",
       " 'longer': 662,\n",
       " 'sent': 663,\n",
       " 'helpful': 664,\n",
       " 'finish': 665,\n",
       " 'spent': 666,\n",
       " 'soon': 667,\n",
       " 'writer': 668,\n",
       " '20': 669,\n",
       " 'audio': 670,\n",
       " 'room': 671,\n",
       " 'software': 672,\n",
       " 'e': 673,\n",
       " 'blu': 674,\n",
       " 'pieces': 675,\n",
       " 'plastic': 676,\n",
       " 'spend': 677,\n",
       " 'extra': 678,\n",
       " 'lives': 679,\n",
       " 'tape': 680,\n",
       " 'junk': 681,\n",
       " 'shipping': 682,\n",
       " 'forward': 683,\n",
       " 'useful': 684,\n",
       " 'actors': 685,\n",
       " 'fall': 686,\n",
       " 'library': 687,\n",
       " 'unit': 688,\n",
       " 'attention': 689,\n",
       " 'matter': 690,\n",
       " 'example': 691,\n",
       " '30': 692,\n",
       " 'otherwise': 693,\n",
       " 'annoying': 694,\n",
       " 'charge': 695,\n",
       " 'features': 696,\n",
       " 'certainly': 697,\n",
       " 'message': 698,\n",
       " 'albums': 699,\n",
       " 'boots': 700,\n",
       " 'interest': 701,\n",
       " 'hand': 702,\n",
       " 'white': 703,\n",
       " 'except': 704,\n",
       " 'control': 705,\n",
       " 'happened': 706,\n",
       " 'open': 707,\n",
       " 'group': 708,\n",
       " 'broke': 709,\n",
       " 'taken': 710,\n",
       " 'humor': 711,\n",
       " '15': 712,\n",
       " 'water': 713,\n",
       " 'previous': 714,\n",
       " 'apart': 715,\n",
       " 'against': 716,\n",
       " 'anyway': 717,\n",
       " 'pop': 718,\n",
       " 'plus': 719,\n",
       " 'c': 720,\n",
       " 'thank': 721,\n",
       " 'figure': 722,\n",
       " 'performance': 723,\n",
       " 'hate': 724,\n",
       " 'none': 725,\n",
       " '100': 726,\n",
       " 'beyond': 727,\n",
       " 'general': 728,\n",
       " 'dog': 729,\n",
       " 'brother': 730,\n",
       " 'guys': 731,\n",
       " 'actual': 732,\n",
       " 'adapter': 733,\n",
       " 'wonder': 734,\n",
       " 'suggest': 735,\n",
       " 'miss': 736,\n",
       " 'pick': 737,\n",
       " '451': 738,\n",
       " 'inside': 739,\n",
       " 'bottom': 740,\n",
       " 'happen': 741,\n",
       " 'turned': 742,\n",
       " 'system': 743,\n",
       " 'face': 744,\n",
       " 'novels': 745,\n",
       " 'paid': 746,\n",
       " 'agree': 747,\n",
       " 'known': 748,\n",
       " 'call': 749,\n",
       " 'test': 750,\n",
       " 'enjoyable': 751,\n",
       " 're': 752,\n",
       " 'due': 753,\n",
       " 'guide': 754,\n",
       " 'hour': 755,\n",
       " 'possible': 756,\n",
       " 'content': 757,\n",
       " 'exactly': 758,\n",
       " 'break': 759,\n",
       " 'blue': 760,\n",
       " 'returned': 761,\n",
       " 'space': 762,\n",
       " 'country': 763,\n",
       " 'decent': 764,\n",
       " 'creative': 765,\n",
       " 'record': 766,\n",
       " 'graphics': 767,\n",
       " 'readers': 768,\n",
       " 'expensive': 769,\n",
       " 'death': 770,\n",
       " 'wife': 771,\n",
       " '7': 772,\n",
       " \"they're\": 773,\n",
       " 'plug': 774,\n",
       " 'seller': 775,\n",
       " 'hoping': 776,\n",
       " 'sleep': 777,\n",
       " 'coming': 778,\n",
       " 'condition': 779,\n",
       " 'cable': 780,\n",
       " 'personal': 781,\n",
       " 'feeling': 782,\n",
       " 'leave': 783,\n",
       " 'included': 784,\n",
       " 'basic': 785,\n",
       " 'metal': 786,\n",
       " 'apple': 787,\n",
       " 'throughout': 788,\n",
       " 'clearly': 789,\n",
       " 'bother': 790,\n",
       " 'thanks': 791,\n",
       " 'brand': 792,\n",
       " 'greatest': 793,\n",
       " 'com': 794,\n",
       " 'stopped': 795,\n",
       " 'total': 796,\n",
       " 'sex': 797,\n",
       " 'effort': 798,\n",
       " 'crap': 799,\n",
       " 'quick': 800,\n",
       " 'season': 801,\n",
       " 'detail': 802,\n",
       " 'released': 803,\n",
       " 'giving': 804,\n",
       " 'wants': 805,\n",
       " 'confusing': 806,\n",
       " 'mine': 807,\n",
       " 'issues': 808,\n",
       " 'stand': 809,\n",
       " 'design': 810,\n",
       " 'beat': 811,\n",
       " 'production': 812,\n",
       " 'chance': 813,\n",
       " 'fire': 814,\n",
       " 'skin': 815,\n",
       " 'move': 816,\n",
       " 'hit': 817,\n",
       " 'products': 818,\n",
       " 'concert': 819,\n",
       " 'canon': 820,\n",
       " 'knew': 821,\n",
       " 'dead': 822,\n",
       " 'eyes': 823,\n",
       " 'lack': 824,\n",
       " 'sony': 825,\n",
       " 'major': 826,\n",
       " 'uses': 827,\n",
       " '9': 828,\n",
       " 'la': 829,\n",
       " 'somewhat': 830,\n",
       " 'foundation': 831,\n",
       " 'fell': 832,\n",
       " 'fascinating': 833,\n",
       " 'seeing': 834,\n",
       " 'middle': 835,\n",
       " 'third': 836,\n",
       " 'rice': 837,\n",
       " 'cave': 838,\n",
       " 'learned': 839,\n",
       " 'men': 840,\n",
       " 'surprised': 841,\n",
       " 'romance': 842,\n",
       " 'boy': 843,\n",
       " 'addition': 844,\n",
       " 'dull': 845,\n",
       " 'subject': 846,\n",
       " 'thin': 847,\n",
       " 'plain': 848,\n",
       " 'baby': 849,\n",
       " 'waiting': 850,\n",
       " 'starts': 851,\n",
       " 'hp': 852,\n",
       " 'twice': 853,\n",
       " 'talking': 854,\n",
       " 'older': 855,\n",
       " 'excited': 856,\n",
       " \"he's\": 857,\n",
       " 'talk': 858,\n",
       " 'despite': 859,\n",
       " 'note': 860,\n",
       " 'jack': 861,\n",
       " 'sort': 862,\n",
       " 'wow': 863,\n",
       " 'manson': 864,\n",
       " 'beware': 865,\n",
       " 'behind': 866,\n",
       " 'b': 867,\n",
       " 'running': 868,\n",
       " 'ended': 869,\n",
       " 'similar': 870,\n",
       " 'basically': 871,\n",
       " 'body': 872,\n",
       " 'broken': 873,\n",
       " 'orwell': 874,\n",
       " 'forget': 875,\n",
       " 'send': 876,\n",
       " 'pair': 877,\n",
       " 'memory': 878,\n",
       " 'incredible': 879,\n",
       " 'historical': 880,\n",
       " '0': 881,\n",
       " '50': 882,\n",
       " 'serious': 883,\n",
       " 'avoid': 884,\n",
       " 'de': 885,\n",
       " 'warning': 886,\n",
       " 'including': 887,\n",
       " 'amount': 888,\n",
       " 'genre': 889,\n",
       " 'parents': 890,\n",
       " 'knowledge': 891,\n",
       " 'bradbury': 892,\n",
       " 'soundtrack': 893,\n",
       " 'brilliant': 894,\n",
       " 'useless': 895,\n",
       " 'max': 896,\n",
       " 'list': 897,\n",
       " 'boot': 898,\n",
       " 'cast': 899,\n",
       " 'super': 900,\n",
       " 'rating': 901,\n",
       " 'customer': 902,\n",
       " 'truth': 903,\n",
       " 'stick': 904,\n",
       " 'fahrenheit': 905,\n",
       " 'america': 906,\n",
       " 'late': 907,\n",
       " 'reference': 908,\n",
       " 'moving': 909,\n",
       " 'skip': 910,\n",
       " 'trip': 911,\n",
       " 'living': 912,\n",
       " 'government': 913,\n",
       " 'okay': 914,\n",
       " 'hands': 915,\n",
       " 'finished': 916,\n",
       " 'alot': 917,\n",
       " 'car': 918,\n",
       " 'lyrics': 919,\n",
       " 'weight': 920,\n",
       " 'date': 921,\n",
       " 'radio': 922,\n",
       " 'students': 923,\n",
       " 'fi': 924,\n",
       " 'themselves': 925,\n",
       " 'waist': 926,\n",
       " 'stuck': 927,\n",
       " 'transformers': 928,\n",
       " 'ms': 929,\n",
       " 'digital': 930,\n",
       " 'insight': 931,\n",
       " 'pass': 932,\n",
       " '12': 933,\n",
       " 'impressed': 934,\n",
       " 'trash': 935,\n",
       " 'heavy': 936,\n",
       " 'changed': 937,\n",
       " 'weak': 938,\n",
       " 'events': 939,\n",
       " 'seriously': 940,\n",
       " 'replace': 941,\n",
       " 'clean': 942,\n",
       " 'shame': 943,\n",
       " 'authors': 944,\n",
       " 'questions': 945,\n",
       " 'correct': 946,\n",
       " 'mother': 947,\n",
       " 'study': 948,\n",
       " 'happens': 949,\n",
       " 'pleased': 950,\n",
       " 'keeps': 951,\n",
       " 'mix': 952,\n",
       " 'model': 953,\n",
       " 'form': 954,\n",
       " 'himself': 955,\n",
       " 'adventure': 956,\n",
       " 'informative': 957,\n",
       " 'shot': 958,\n",
       " 'recently': 959,\n",
       " 'haunting': 960,\n",
       " 'windows': 961,\n",
       " 'century': 962,\n",
       " 'alone': 963,\n",
       " 'bored': 964,\n",
       " 'exciting': 965,\n",
       " 'cute': 966,\n",
       " 'cause': 967,\n",
       " 'development': 968,\n",
       " 'details': 969,\n",
       " 'putting': 970,\n",
       " 'crazy': 971,\n",
       " 'masterpiece': 972,\n",
       " 'imagine': 973,\n",
       " 'grade': 974,\n",
       " 'straight': 975,\n",
       " 'ridiculous': 976,\n",
       " 'background': 977,\n",
       " 'flat': 978,\n",
       " 'across': 979,\n",
       " 'research': 980,\n",
       " 'issue': 981,\n",
       " 'appears': 982,\n",
       " 'sci': 983,\n",
       " 'learning': 984,\n",
       " 'advice': 985,\n",
       " 'front': 986,\n",
       " 'wasted': 987,\n",
       " 'forever': 988,\n",
       " 'mattress': 989,\n",
       " 'means': 990,\n",
       " 'turns': 991,\n",
       " 'comedy': 992,\n",
       " 'difference': 993,\n",
       " 'nearly': 994,\n",
       " 'reviewers': 995,\n",
       " 'incredibly': 996,\n",
       " 'recording': 997,\n",
       " 'obviously': 998,\n",
       " 'business': 999,\n",
       " 'introduction': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(x_train), maxlen=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(x_valid), maxlen=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-trained word embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_vectors = open('wiki-news-300d-1M.vec', 'r', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(pre_trained_vectors):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre_trained_vectors = open('wiki-news-300d-1M.vec', 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('not', array([ 1.570e-02, -7.480e-02, -8.620e-02,  5.040e-02, -3.800e-03,\n",
      "        4.300e-02, -3.580e-02, -3.290e-02,  4.210e-02,  5.400e-03,\n",
      "        2.210e-02,  6.220e-02,  5.860e-02,  4.820e-02, -7.620e-02,\n",
      "       -6.700e-03,  5.320e-02,  1.316e-01,  1.638e-01, -5.680e-02,\n",
      "       -3.210e-02, -1.290e-02, -6.000e-04,  1.519e-01, -6.130e-02,\n",
      "        5.080e-02,  3.810e-02,  2.320e-02, -3.780e-02,  2.808e-01,\n",
      "        5.980e-02, -1.570e-02,  1.495e-01,  7.000e-04, -3.200e-03,\n",
      "        2.530e-02,  6.230e-02,  2.170e-02,  5.250e-02,  9.500e-03,\n",
      "       -5.700e-02,  4.360e-02,  4.400e-02,  2.740e-02, -2.390e-02,\n",
      "       -3.900e-02, -5.570e-02,  2.120e-02,  2.000e-02,  7.500e-03,\n",
      "       -1.580e-02,  3.930e-02, -6.234e-01, -3.500e-02,  8.600e-03,\n",
      "        1.540e-02,  5.230e-02, -6.900e-02,  1.258e-01,  2.350e-02,\n",
      "        5.100e-03,  2.200e-02, -4.300e-02, -9.200e-03,  7.980e-02,\n",
      "        1.110e-02,  5.500e-03,  4.560e-02, -2.220e-02,  3.640e-02,\n",
      "        3.560e-02, -1.050e-02, -3.780e-02,  1.604e-01,  1.930e-02,\n",
      "        4.810e-02, -3.450e-02,  1.640e-02,  5.840e-02,  2.579e-01,\n",
      "        5.250e-02, -1.140e-02,  7.000e-03, -1.814e-01,  2.700e-03,\n",
      "        2.450e-02, -4.320e-02, -3.100e-03, -3.724e-01,  2.000e-02,\n",
      "        2.000e-02, -4.030e-02, -2.440e-02, -9.050e-02, -7.400e-03,\n",
      "        5.740e-02,  1.670e-02, -5.600e-03,  2.830e-02,  2.500e-02,\n",
      "       -1.321e-01,  7.500e-03, -8.300e-03, -2.790e-02,  1.681e-01,\n",
      "        8.400e-03,  2.149e-01, -2.190e-02,  4.980e-02, -3.130e-02,\n",
      "       -4.820e-02, -5.690e-02, -4.900e-03, -1.670e-02,  2.380e-02,\n",
      "       -6.780e-02, -2.830e-02, -4.580e-02,  4.540e-02, -3.103e-01,\n",
      "       -1.018e-01, -7.740e-02,  3.120e-02,  4.160e-02, -6.830e-02,\n",
      "        1.019e-01, -8.430e-02,  6.090e-02, -9.390e-02, -4.660e-02,\n",
      "        3.100e-03, -1.770e-02, -7.170e-02, -6.300e-03,  4.610e-02,\n",
      "       -2.670e-01,  5.300e-03, -5.380e-02,  4.050e-02,  2.500e-03,\n",
      "       -3.350e-02,  1.090e-02, -1.740e-02,  3.076e-01,  5.250e-02,\n",
      "       -2.610e-02,  7.130e-02, -6.080e-02, -4.750e-02, -2.310e-02,\n",
      "       -7.750e-02,  4.060e-02,  3.440e-02, -3.060e-02,  2.630e-02,\n",
      "        2.250e-02, -1.200e-02, -5.290e-02,  1.990e-02, -3.980e-02,\n",
      "        3.620e-02,  3.970e-02,  9.900e-03, -7.300e-02, -2.366e-01,\n",
      "       -4.120e-02,  8.470e-02,  1.102e-01,  2.900e-03, -2.260e-02,\n",
      "        8.060e-02,  1.760e-02,  5.460e-02, -3.020e-02,  1.086e-01,\n",
      "       -3.090e-02,  1.505e-01, -1.936e-01,  5.900e-02,  2.920e-02,\n",
      "        4.790e-02, -5.970e-02, -5.970e-02,  4.100e-03, -5.730e-02,\n",
      "       -5.370e-02,  7.000e-04, -1.571e-01,  2.532e-01,  1.006e-01,\n",
      "       -1.350e-02,  3.300e-03,  1.700e-02, -1.810e-02, -2.020e-02,\n",
      "       -2.480e-02,  3.420e-02, -4.380e-02,  2.989e-01,  3.350e-02,\n",
      "        5.990e-02, -4.550e-02, -6.400e-03, -2.250e-02, -1.060e-02,\n",
      "       -3.150e-02, -9.100e-03,  1.720e-02,  1.190e-02, -2.050e-02,\n",
      "        2.390e-02,  1.250e-02, -6.240e-02, -2.240e-02, -1.720e-02,\n",
      "        1.670e-02,  8.200e-03,  1.180e-02,  1.200e-02, -6.100e-03,\n",
      "        2.850e-02, -1.820e-02, -9.300e-03,  1.610e-02, -2.590e-02,\n",
      "       -2.024e-01, -8.400e-03,  6.090e-02,  1.800e-03, -1.211e-01,\n",
      "        4.410e-02, -5.190e-02,  1.868e-01, -4.350e-02,  3.640e-02,\n",
      "       -7.860e-02,  7.640e-02, -4.700e-03, -1.362e-01,  5.960e-02,\n",
      "       -2.180e-02, -3.600e-03,  1.460e-02,  1.020e-02, -5.660e-02,\n",
      "       -7.490e-02,  1.850e-02, -3.600e-03,  8.520e-02,  3.222e-01,\n",
      "       -4.150e-02, -5.450e-02, -9.790e-02,  3.090e-02,  1.487e-01,\n",
      "        2.650e-02,  1.270e-02, -4.150e-02, -4.590e-02, -2.980e-02,\n",
      "       -4.600e-02,  7.000e-04,  3.600e-03, -4.060e-02, -3.001e-01,\n",
      "        2.370e-02, -1.201e-01, -3.550e-02, -9.600e-02, -1.060e-02,\n",
      "       -3.280e-02, -4.760e-02, -5.900e-03, -2.680e-02, -1.870e-02,\n",
      "        5.500e-02, -3.190e-02, -1.211e-01,  3.190e-02, -8.000e-03,\n",
      "        2.461e-01,  1.684e-01,  1.880e-02, -4.200e-02,  9.490e-02,\n",
      "       -1.080e-02,  3.890e-02,  7.560e-02, -8.980e-02,  2.880e-02,\n",
      "        3.050e-02, -3.390e-02,  3.300e-02, -2.600e-03, -3.950e-02,\n",
      "       -5.770e-02, -6.000e-02,  1.890e-01, -4.330e-02, -1.014e-01],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(list(embeddings_index.items())[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EMBEDDING MATRIX represents the vector for each word in the entire text indexed by word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.11600000e-01,  8.56000036e-02, -6.89999992e-03, -1.79700002e-01,\n",
       "        8.54000002e-02, -6.08999990e-02,  6.25000000e-02,  1.22800000e-01,\n",
       "        1.80700004e-01,  7.63999969e-02, -2.49000005e-02, -5.24999984e-02,\n",
       "       -9.99999975e-05, -1.99300006e-01, -1.59700006e-01, -8.07999969e-02,\n",
       "       -7.31000006e-02, -4.50000018e-02, -1.23300001e-01, -3.97000015e-02,\n",
       "        1.49999997e-02, -1.38099998e-01, -4.82000001e-02,  5.62000014e-02,\n",
       "       -9.74999964e-02, -1.27399996e-01,  3.95000018e-02,  2.75599986e-01,\n",
       "        6.80000037e-02, -1.41800001e-01,  2.72399992e-01, -8.89999978e-03,\n",
       "        9.21000019e-02,  1.52799994e-01,  2.19999999e-01,  1.84000004e-02,\n",
       "       -5.13999984e-02,  1.32499993e-01,  1.40200004e-01, -1.00000005e-03,\n",
       "        2.67999992e-02,  5.15000001e-02, -7.75000006e-02, -5.22000007e-02,\n",
       "       -1.18000004e-02,  3.89999989e-03, -2.14900002e-01,  1.01700000e-01,\n",
       "        1.93200007e-01, -6.59999996e-02, -6.58999979e-02,  5.46999983e-02,\n",
       "       -6.58299983e-01, -2.51100004e-01, -2.04000007e-02,  6.75999969e-02,\n",
       "       -9.18999985e-02, -1.42299995e-01,  7.69999996e-02, -3.94000001e-02,\n",
       "        6.26000017e-02, -7.71000013e-02,  9.35999975e-02,  1.80999994e-01,\n",
       "       -5.13000004e-02, -9.64000002e-02, -3.84000018e-02,  2.89999992e-02,\n",
       "       -9.39999986e-03,  1.17799997e-01, -7.24999979e-02, -1.41000003e-02,\n",
       "       -8.56000036e-02,  1.17299996e-01,  1.61699995e-01,  7.15000033e-02,\n",
       "       -1.28700003e-01, -2.06000004e-02,  3.28000002e-02,  1.33100003e-01,\n",
       "        2.88999993e-02, -1.00100003e-01, -6.98999986e-02, -1.56299993e-01,\n",
       "       -2.08000001e-02, -3.29000019e-02, -5.73999994e-02,  1.53999999e-02,\n",
       "       -2.88599998e-01, -5.46999983e-02, -3.18999998e-02,  4.65000011e-02,\n",
       "       -5.13999984e-02, -1.11299999e-01,  4.63999994e-02, -2.85999998e-02,\n",
       "       -5.40000014e-03,  5.15000001e-02,  5.97999990e-02, -4.49999981e-03,\n",
       "       -3.37700009e-01, -2.89999992e-02,  2.85000000e-02, -9.75999981e-02,\n",
       "       -3.75699997e-01,  7.49000013e-02, -7.89000019e-02,  7.10000023e-02,\n",
       "       -9.56000015e-02,  9.30000003e-03, -5.07999994e-02,  4.94000018e-02,\n",
       "        1.31300002e-01,  1.44800007e-01, -2.64699996e-01,  1.45600006e-01,\n",
       "       -1.27800003e-01, -6.69000000e-02, -1.25799999e-01, -2.48600006e-01,\n",
       "       -8.39999970e-03,  9.70999971e-02, -1.68500006e-01, -1.15400001e-01,\n",
       "       -2.01000005e-01,  8.83999988e-02,  2.51800001e-01,  8.13999996e-02,\n",
       "       -7.81000033e-02, -5.18000014e-02, -7.47999996e-02,  3.70000005e-02,\n",
       "       -3.99999990e-04,  1.30999997e-01,  3.19999992e-03, -2.64099985e-01,\n",
       "       -9.13000032e-02, -8.86000022e-02,  1.70000002e-03,  4.30999994e-02,\n",
       "       -7.22000003e-02,  3.09999995e-02, -3.42599988e-01,  1.81999996e-01,\n",
       "        1.81000009e-02, -6.61000013e-02,  2.78800011e-01,  1.00599997e-01,\n",
       "        4.39999998e-03, -5.05000018e-02,  2.96999998e-02,  1.40699998e-01,\n",
       "        8.44999999e-02,  3.28000002e-02, -1.08900003e-01,  8.79999995e-02,\n",
       "       -1.12499997e-01,  5.29999984e-03, -1.75999999e-01, -1.33200005e-01,\n",
       "        3.77000012e-02, -6.23999983e-02,  1.07500002e-01,  1.43600002e-01,\n",
       "       -2.06200004e-01,  1.89799994e-01,  1.02799997e-01, -7.67000020e-02,\n",
       "        3.48999985e-02,  3.51999998e-02, -1.79499999e-01, -3.50000011e-03,\n",
       "       -1.53999999e-01, -3.97000015e-02,  8.35999995e-02,  6.12999983e-02,\n",
       "        3.46300006e-01,  6.57599986e-01,  9.01999995e-02, -1.05499998e-01,\n",
       "       -4.25999984e-02,  6.88999966e-02, -2.16000006e-02,  7.68000036e-02,\n",
       "       -8.39999989e-02, -1.73999995e-01, -1.29899994e-01, -1.55000001e-01,\n",
       "       -1.56700000e-01,  8.60000029e-02,  1.70000009e-02,  6.88000023e-02,\n",
       "       -5.48000000e-02, -2.49000005e-02, -3.29000019e-02, -1.33100003e-01,\n",
       "       -4.17999998e-02, -5.66000007e-02, -5.79999983e-02,  2.49000005e-02,\n",
       "        1.34900004e-01,  4.03000005e-02, -5.29999994e-02, -4.87999991e-02,\n",
       "       -4.01000008e-02, -1.79999997e-03, -4.47000004e-02,  1.62000004e-02,\n",
       "        3.20999995e-02, -2.00999994e-02,  1.03299998e-01, -7.13000000e-02,\n",
       "        1.12800002e-01, -1.23400003e-01, -2.85000000e-02, -1.42199993e-01,\n",
       "       -2.13300005e-01, -9.60000046e-03, -8.69000033e-02,  2.20699996e-01,\n",
       "       -4.14299995e-01, -1.50199994e-01, -1.11900002e-01, -3.00000014e-04,\n",
       "       -6.84000030e-02,  6.15000017e-02, -4.69000004e-02,  3.59000005e-02,\n",
       "        7.37999976e-02, -1.33399993e-01,  1.97799996e-01,  4.87000011e-02,\n",
       "        3.06100011e-01, -1.27900004e-01, -1.31999999e-02, -1.02600001e-01,\n",
       "        3.02000009e-02,  1.59000009e-02, -1.90099999e-01, -1.12800002e-01,\n",
       "       -6.58000037e-02, -1.49200007e-01, -4.52000014e-02, -2.39399999e-01,\n",
       "       -2.40000002e-02,  1.37700006e-01,  8.46000016e-02, -1.79999997e-03,\n",
       "        3.55999991e-02,  2.58800000e-01,  1.11000001e-01, -3.42000015e-02,\n",
       "        2.47799993e-01,  1.11400001e-01,  1.34499997e-01, -6.06999993e-02,\n",
       "       -3.79000008e-02,  8.03999975e-02, -1.41399994e-01,  3.80000006e-03,\n",
       "        2.61000004e-02, -7.25999996e-02, -7.30000017e-03,  3.89999989e-03,\n",
       "       -5.83599985e-01, -1.81000009e-02, -1.42800003e-01, -5.93000017e-02,\n",
       "        6.21999986e-02, -1.70300007e-01,  1.43999998e-02,  2.39000004e-02,\n",
       "        1.14600003e-01,  4.80000004e-02, -2.96999998e-02,  1.35900006e-01,\n",
       "        8.99999961e-03, -5.93999997e-02,  7.31000006e-02,  6.19000010e-02,\n",
       "        1.33100003e-01,  1.00199997e-01,  4.03000005e-02,  2.91000009e-02,\n",
       "        7.42999986e-02, -7.50000030e-02,  1.19300000e-01, -5.49999997e-02,\n",
       "       -1.11999996e-02,  5.02000004e-02, -7.79999979e-03,  7.36000016e-02,\n",
       "        7.72999972e-02,  1.35100007e-01,  2.97999997e-02,  3.22999991e-02,\n",
       "       -1.14000002e-02,  8.77000019e-02,  1.01899996e-01,  9.70000029e-03])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic Modelling Feature using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_topics = lda_model.fit_transform(x_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = lda_model.components_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = numpy.array(terms)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['concert unbelievable meant toys emotion area twists birthday leads diary',\n",
       " 'charger apple 3d letter hawthorne 00 scarlet scarlet letter cornwell scarpetta',\n",
       " 'century version tool standard versions cat originally shower smell extreme',\n",
       " 'printer hp paper stargate print color poor quality photo quality poor',\n",
       " 'recipes cooking concept coast ingredients portrayal lee thrash dishes worlds',\n",
       " 'hot rice bands customer service delightful passion costume customer taste whatsoever',\n",
       " 'catholic unusual combination church freud dragon castle dickens funk arguments',\n",
       " 'card battery product blu amazon blu ray ray boots camera player',\n",
       " '1 3 size 2 5 max 4 waist product internet',\n",
       " 'great cd glass cake ice 18 troma guest guns barbie string',\n",
       " 'la bar y en spanish el que stopped working blocks album',\n",
       " 'album cd music songs albums band sex tracks listen rock',\n",
       " 'scanner driver solution great price magazine mrs europe japanese scan xp',\n",
       " 'book s t good like great read just time movie',\n",
       " 'movie video road diane lane diane lane yoga van kerouac walk',\n",
       " 'charge power adapter t wait economics bottle charging highly recommended g4 product',\n",
       " 'movie effects horror special game special effects movies scary acting jones',\n",
       " 'bowie chocolate pepper require news newspaper pants dont buy miller assume',\n",
       " 'fit ear jawbone use product comfortable practice t fit jabra term',\n",
       " 'book read books bed 1984 orwell air society fiction 451']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = pd.DataFrame(data=X_topics).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.767915</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.017236</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.194293</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.363745</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.598755</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.002083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.802449</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.187976</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.475098</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.468652</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.041536</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.390607</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.592696</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.402567</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.334098</td>\n",
       "      <td>0.651839</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.462842</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.516703</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.027632</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.027632</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.328454</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.595230</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.590389</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.381486</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.000556  0.000556  0.000556  0.000556  0.000556  0.000556  0.000556   \n",
       "1  0.002083  0.002083  0.002083  0.002083  0.002083  0.002083  0.002083   \n",
       "2  0.000532  0.000532  0.000532  0.000532  0.000532  0.000532  0.000532   \n",
       "3  0.003125  0.003125  0.003125  0.003125  0.003125  0.003125  0.475098   \n",
       "4  0.001786  0.001786  0.001786  0.001786  0.001786  0.001786  0.001786   \n",
       "5  0.000263  0.000263  0.000263  0.000263  0.000263  0.000263  0.000263   \n",
       "6  0.000781  0.000781  0.000781  0.000781  0.000781  0.000781  0.000781   \n",
       "7  0.001136  0.001136  0.001136  0.001136  0.001136  0.001136  0.001136   \n",
       "8  0.001316  0.001316  0.001316  0.001316  0.027632  0.001316  0.001316   \n",
       "9  0.001563  0.590389  0.001563  0.001563  0.001563  0.001563  0.001563   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0  0.767915  0.011667  0.000556  0.000556  0.017236  0.000556  0.194293   \n",
       "1  0.002083  0.363745  0.002083  0.002083  0.002083  0.002083  0.598755   \n",
       "2  0.802449  0.000532  0.000532  0.000532  0.000532  0.000532  0.187976   \n",
       "3  0.003125  0.003125  0.003125  0.003125  0.003125  0.003125  0.468652   \n",
       "4  0.001786  0.041536  0.001786  0.001786  0.001786  0.537500  0.390607   \n",
       "5  0.000263  0.000263  0.000263  0.592696  0.000263  0.000263  0.402567   \n",
       "6  0.000781  0.000781  0.000781  0.000781  0.000781  0.000781  0.334098   \n",
       "7  0.462842  0.001136  0.001136  0.001136  0.001136  0.001136  0.516703   \n",
       "8  0.001316  0.027632  0.001316  0.001316  0.001316  0.001316  0.328454   \n",
       "9  0.001563  0.001563  0.001563  0.001563  0.001563  0.001563  0.381486   \n",
       "\n",
       "         14        15        16        17        18        19  \n",
       "0  0.000556  0.000556  0.000556  0.000556  0.000556  0.000556  \n",
       "1  0.002083  0.002083  0.002083  0.002083  0.002083  0.002083  \n",
       "2  0.000532  0.000532  0.000532  0.000532  0.000532  0.000532  \n",
       "3  0.003125  0.003125  0.003125  0.003125  0.003125  0.003125  \n",
       "4  0.001786  0.001786  0.001786  0.001786  0.001786  0.001786  \n",
       "5  0.000263  0.000263  0.000263  0.000263  0.000263  0.000263  \n",
       "6  0.651839  0.000781  0.000781  0.000781  0.000781  0.000781  \n",
       "7  0.001136  0.001136  0.001136  0.001136  0.001136  0.001136  \n",
       "8  0.001316  0.595230  0.001316  0.001316  0.001316  0.001316  \n",
       "9  0.001563  0.001563  0.001563  0.001563  0.001563  0.001563  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     7\n",
       "1    13\n",
       "2     7\n",
       "3     6\n",
       "4    12\n",
       "5    10\n",
       "6    14\n",
       "7    13\n",
       "8    15\n",
       "9     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, x_train_features, x_valid_features, is_neural_net=False):\n",
    "    classifier.fit(x_train_features, y_train)\n",
    "    predictions = classifier.predict(x_valid_features)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_count = train_model(naive_bayes.MultinomialNB(), x_train_count, x_valid_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, count vectors: 0.8548\n"
     ]
    }
   ],
   "source": [
    "print('NB, count vectors: {}'.format(accuracy_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tfidf = train_model(naive_bayes.MultinomialNB(), x_train_tfidf, x_valid_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, tfidf word: 0.8536\n"
     ]
    }
   ],
   "source": [
    "print('NB, tfidf word: {}'.format(accuracy_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_ngram = train_model(naive_bayes.MultinomialNB(), x_train_tfidf_ngram, x_valid_tfidf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, ngram word: 0.8388\n"
     ]
    }
   ],
   "source": [
    "print('NB, ngram word: {}'.format(accuracy_ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_ngram_char = train_model(naive_bayes.MultinomialNB(), x_train_tfidf_char, x_valid_tfidf_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, ngram char: 0.8256\n"
     ]
    }
   ],
   "source": [
    "print('NB, ngram char: {}'.format(accuracy_ngram_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_classifier = naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_classifier.fit(x_train_tfidf_char, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = bayes_classifier.predict(x_valid_tfidf_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear model: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chloris.Yu\\AppData\\Local\\Continuum\\anaconda3\\envs\\hello-tf\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "accuracy_count_LR = train_model(linear_model.LogisticRegression(), x_train_count, x_valid_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vector: 0.85\n"
     ]
    }
   ],
   "source": [
    "print('LR, Count Vector: {}'.format(accuracy_count_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chloris.Yu\\AppData\\Local\\Continuum\\anaconda3\\envs\\hello-tf\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "accuracy_tfidf_svm = train_model(svm.SVC(), x_train_tfidf_ngram, x_valid_tfidf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, TFIDF: 0.5268\n"
     ]
    }
   ],
   "source": [
    "print('SVM, TFIDF: {}'.format(accuracy_tfidf_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chloris.Yu\\AppData\\Local\\Continuum\\anaconda3\\envs\\hello-tf\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "accuracy_tfidf_RF = train_model(ensemble.RandomForestClassifier(), x_train_tfidf_ngram, x_valid_tfidf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, TFIDF: 0.742\n"
     ]
    }
   ],
   "source": [
    "print('RF, TFIDF: {}'.format(accuracy_tfidf_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500,)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241330"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tfidf.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHALLOW NEURAL NETWORK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - ETA: 1:41 - loss: 8.967 - ETA: 22s - loss: 9.565 - ETA: 13s - loss: 8.80 - ETA: 10s - loss: 8.81 - ETA: 8s - loss: 8.8211 - ETA: 7s - loss: 8.611 - ETA: 6s - loss: 8.788 - ETA: 5s - loss: 8.795 - ETA: 5s - loss: 8.500 - ETA: 5s - loss: 8.427 - ETA: 4s - loss: 8.332 - ETA: 4s - loss: 8.318 - ETA: 4s - loss: 8.328 - ETA: 4s - loss: 8.280 - ETA: 4s - loss: 8.229 - ETA: 3s - loss: 8.194 - ETA: 3s - loss: 8.148 - ETA: 3s - loss: 8.175 - ETA: 3s - loss: 8.142 - ETA: 3s - loss: 8.112 - ETA: 3s - loss: 8.124 - ETA: 3s - loss: 8.104 - ETA: 2s - loss: 8.104 - ETA: 2s - loss: 8.093 - ETA: 2s - loss: 8.071 - ETA: 2s - loss: 8.022 - ETA: 2s - loss: 8.034 - ETA: 2s - loss: 8.018 - ETA: 2s - loss: 8.007 - ETA: 2s - loss: 8.019 - ETA: 2s - loss: 8.038 - ETA: 2s - loss: 8.012 - ETA: 1s - loss: 8.006 - ETA: 1s - loss: 7.952 - ETA: 1s - loss: 7.945 - ETA: 1s - loss: 7.927 - ETA: 1s - loss: 7.946 - ETA: 1s - loss: 7.947 - ETA: 1s - loss: 7.947 - ETA: 1s - loss: 7.951 - ETA: 1s - loss: 7.961 - ETA: 1s - loss: 7.971 - ETA: 1s - loss: 7.986 - ETA: 1s - loss: 8.012 - ETA: 1s - loss: 8.020 - ETA: 0s - loss: 8.002 - ETA: 0s - loss: 8.012 - ETA: 0s - loss: 8.000 - ETA: 0s - loss: 8.003 - ETA: 0s - loss: 7.997 - ETA: 0s - loss: 7.991 - ETA: 0s - loss: 7.993 - ETA: 0s - loss: 8.007 - ETA: 0s - loss: 7.985 - ETA: 0s - loss: 8.001 - ETA: 0s - loss: 7.987 - ETA: 0s - loss: 8.005 - ETA: 0s - loss: 8.015 - ETA: 0s - loss: 8.043 - ETA: 0s - loss: 8.033 - 4s 522us/step - loss: 8.0350\n",
      "NN, tfidf-ngram: 0.5268\n"
     ]
    }
   ],
   "source": [
    "def create_model_architecture(input_size):\n",
    "    #input layer\n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    #hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation='relu')(input_layer)\n",
    "    \n",
    "    #output_layer\n",
    "    output_layer = layers.Dense(1, activation='softmax')(hidden_layer)\n",
    "    \n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(lr=0.01 ), loss='binary_crossentropy')\n",
    "    \n",
    "    return classifier\n",
    "    \n",
    "classifier = create_model_architecture(x_train_tfidf_ngram.shape[1])\n",
    "accuracy = train_model(classifier,x_train_tfidf_ngram, x_valid_tfidf_ngram, is_neural_net=True )\n",
    "print('NN, tfidf-ngram: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Chloris.Yu\\AppData\\Local\\Continuum\\anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - ETA: 5:55 - loss: 0.661 - ETA: 2:02 - loss: 0.682 - ETA: 1:15 - loss: 0.686 - ETA: 55s - loss: 0.702 - ETA: 44s - loss: 0.70 - ETA: 37s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 12s - loss: 0.68 - ETA: 12s - loss: 0.68 - ETA: 11s - loss: 0.68 - ETA: 11s - loss: 0.68 - ETA: 11s - loss: 0.68 - ETA: 10s - loss: 0.68 - ETA: 10s - loss: 0.68 - ETA: 10s - loss: 0.68 - ETA: 9s - loss: 0.6867 - ETA: 9s - loss: 0.686 - ETA: 9s - loss: 0.685 - ETA: 9s - loss: 0.684 - ETA: 8s - loss: 0.684 - ETA: 8s - loss: 0.682 - ETA: 8s - loss: 0.681 - ETA: 8s - loss: 0.680 - ETA: 7s - loss: 0.678 - ETA: 7s - loss: 0.678 - ETA: 7s - loss: 0.677 - ETA: 7s - loss: 0.675 - ETA: 7s - loss: 0.675 - ETA: 6s - loss: 0.673 - ETA: 6s - loss: 0.672 - ETA: 6s - loss: 0.670 - ETA: 6s - loss: 0.668 - ETA: 6s - loss: 0.667 - ETA: 6s - loss: 0.666 - ETA: 6s - loss: 0.665 - ETA: 5s - loss: 0.662 - ETA: 5s - loss: 0.662 - ETA: 5s - loss: 0.660 - ETA: 5s - loss: 0.659 - ETA: 5s - loss: 0.658 - ETA: 5s - loss: 0.656 - ETA: 5s - loss: 0.655 - ETA: 4s - loss: 0.654 - ETA: 4s - loss: 0.652 - ETA: 4s - loss: 0.652 - ETA: 4s - loss: 0.652 - ETA: 4s - loss: 0.650 - ETA: 4s - loss: 0.648 - ETA: 4s - loss: 0.646 - ETA: 4s - loss: 0.645 - ETA: 4s - loss: 0.643 - ETA: 4s - loss: 0.641 - ETA: 3s - loss: 0.639 - ETA: 3s - loss: 0.637 - ETA: 3s - loss: 0.635 - ETA: 3s - loss: 0.632 - ETA: 3s - loss: 0.629 - ETA: 3s - loss: 0.627 - ETA: 3s - loss: 0.624 - ETA: 3s - loss: 0.621 - ETA: 2s - loss: 0.619 - ETA: 2s - loss: 0.617 - ETA: 2s - loss: 0.616 - ETA: 2s - loss: 0.614 - ETA: 2s - loss: 0.611 - ETA: 2s - loss: 0.609 - ETA: 2s - loss: 0.606 - ETA: 2s - loss: 0.603 - ETA: 1s - loss: 0.600 - ETA: 1s - loss: 0.596 - ETA: 1s - loss: 0.594 - ETA: 1s - loss: 0.591 - ETA: 1s - loss: 0.589 - ETA: 1s - loss: 0.588 - ETA: 1s - loss: 0.586 - ETA: 1s - loss: 0.584 - ETA: 1s - loss: 0.581 - ETA: 0s - loss: 0.580 - ETA: 0s - loss: 0.578 - ETA: 0s - loss: 0.576 - ETA: 0s - loss: 0.575 - ETA: 0s - loss: 0.574 - ETA: 0s - loss: 0.572 - ETA: 0s - loss: 0.572 - ETA: 0s - loss: 0.572 - ETA: 0s - loss: 0.570 - ETA: 0s - loss: 0.566 - ETA: 0s - loss: 0.565 - ETA: 0s - loss: 0.563 - 7s 979us/step - loss: 0.5624\n",
      "CNN, Word Embeddings 0.5268\n"
     ]
    }
   ],
   "source": [
    "def create_cnn():\n",
    "    \n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "    \n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "    \n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "    \n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "    \n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn()\n",
    "accuracy = train_model(classifier, train_seq_x, valid_seq_x, is_neural_net=True)\n",
    "print (\"CNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - ETA: 8:00 - loss: 0.694 - ETA: 4:10 - loss: 0.701 - ETA: 2:53 - loss: 0.693 - ETA: 2:14 - loss: 0.693 - ETA: 1:50 - loss: 0.695 - ETA: 1:35 - loss: 0.692 - ETA: 1:23 - loss: 0.691 - ETA: 1:15 - loss: 0.689 - ETA: 1:08 - loss: 0.690 - ETA: 1:03 - loss: 0.692 - ETA: 59s - loss: 0.691 - ETA: 55s - loss: 0.68 - ETA: 52s - loss: 0.68 - ETA: 50s - loss: 0.68 - ETA: 47s - loss: 0.68 - ETA: 45s - loss: 0.68 - ETA: 43s - loss: 0.68 - ETA: 42s - loss: 0.68 - ETA: 40s - loss: 0.68 - ETA: 39s - loss: 0.68 - ETA: 38s - loss: 0.68 - ETA: 37s - loss: 0.69 - ETA: 36s - loss: 0.69 - ETA: 35s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 33s - loss: 0.69 - ETA: 33s - loss: 0.69 - ETA: 32s - loss: 0.68 - ETA: 31s - loss: 0.68 - ETA: 31s - loss: 0.68 - ETA: 30s - loss: 0.68 - ETA: 29s - loss: 0.68 - ETA: 29s - loss: 0.68 - ETA: 28s - loss: 0.68 - ETA: 28s - loss: 0.68 - ETA: 27s - loss: 0.68 - ETA: 27s - loss: 0.68 - ETA: 27s - loss: 0.68 - ETA: 26s - loss: 0.68 - ETA: 26s - loss: 0.68 - ETA: 26s - loss: 0.68 - ETA: 25s - loss: 0.68 - ETA: 25s - loss: 0.68 - ETA: 25s - loss: 0.68 - ETA: 24s - loss: 0.68 - ETA: 24s - loss: 0.68 - ETA: 24s - loss: 0.68 - ETA: 23s - loss: 0.68 - ETA: 23s - loss: 0.68 - ETA: 23s - loss: 0.68 - ETA: 22s - loss: 0.68 - ETA: 22s - loss: 0.68 - ETA: 22s - loss: 0.68 - ETA: 22s - loss: 0.68 - ETA: 21s - loss: 0.68 - ETA: 21s - loss: 0.68 - ETA: 21s - loss: 0.68 - ETA: 21s - loss: 0.68 - ETA: 21s - loss: 0.68 - ETA: 20s - loss: 0.68 - ETA: 20s - loss: 0.68 - ETA: 20s - loss: 0.68 - ETA: 20s - loss: 0.68 - ETA: 20s - loss: 0.68 - ETA: 19s - loss: 0.68 - ETA: 19s - loss: 0.68 - ETA: 19s - loss: 0.68 - ETA: 19s - loss: 0.68 - ETA: 19s - loss: 0.68 - ETA: 18s - loss: 0.68 - ETA: 18s - loss: 0.68 - ETA: 18s - loss: 0.68 - ETA: 18s - loss: 0.68 - ETA: 18s - loss: 0.68 - ETA: 18s - loss: 0.68 - ETA: 17s - loss: 0.68 - ETA: 17s - loss: 0.68 - ETA: 17s - loss: 0.68 - ETA: 17s - loss: 0.68 - ETA: 17s - loss: 0.68 - ETA: 17s - loss: 0.68 - ETA: 16s - loss: 0.68 - ETA: 16s - loss: 0.68 - ETA: 16s - loss: 0.68 - ETA: 16s - loss: 0.68 - ETA: 16s - loss: 0.68 - ETA: 16s - loss: 0.68 - ETA: 16s - loss: 0.68 - ETA: 15s - loss: 0.68 - ETA: 15s - loss: 0.68 - ETA: 15s - loss: 0.68 - ETA: 15s - loss: 0.68 - ETA: 15s - loss: 0.68 - ETA: 15s - loss: 0.67 - ETA: 15s - loss: 0.67 - ETA: 15s - loss: 0.67 - ETA: 14s - loss: 0.67 - ETA: 14s - loss: 0.67 - ETA: 14s - loss: 0.67 - ETA: 14s - loss: 0.67 - ETA: 14s - loss: 0.67 - ETA: 14s - loss: 0.67 - ETA: 14s - loss: 0.67 - ETA: 14s - loss: 0.67 - ETA: 13s - loss: 0.67 - ETA: 13s - loss: 0.67 - ETA: 13s - loss: 0.67 - ETA: 13s - loss: 0.67 - ETA: 13s - loss: 0.67 - ETA: 13s - loss: 0.67 - ETA: 13s - loss: 0.67 - ETA: 13s - loss: 0.66 - ETA: 12s - loss: 0.67 - ETA: 12s - loss: 0.67 - ETA: 12s - loss: 0.67 - ETA: 12s - loss: 0.67 - ETA: 12s - loss: 0.67 - ETA: 12s - loss: 0.67 - ETA: 12s - loss: 0.67 - ETA: 12s - loss: 0.67 - ETA: 11s - loss: 0.66 - ETA: 11s - loss: 0.66 - ETA: 11s - loss: 0.66 - ETA: 11s - loss: 0.66 - ETA: 11s - loss: 0.66 - ETA: 11s - loss: 0.66 - ETA: 11s - loss: 0.66 - ETA: 11s - loss: 0.66 - ETA: 10s - loss: 0.66 - ETA: 10s - loss: 0.66 - ETA: 10s - loss: 0.66 - ETA: 10s - loss: 0.66 - ETA: 10s - loss: 0.66 - ETA: 10s - loss: 0.66 - ETA: 10s - loss: 0.66 - ETA: 10s - loss: 0.66 - ETA: 9s - loss: 0.6618 - ETA: 9s - loss: 0.662 - ETA: 9s - loss: 0.661 - ETA: 9s - loss: 0.661 - ETA: 9s - loss: 0.660 - ETA: 9s - loss: 0.661 - ETA: 9s - loss: 0.660 - ETA: 9s - loss: 0.659 - ETA: 9s - loss: 0.658 - ETA: 9s - loss: 0.659 - ETA: 8s - loss: 0.658 - ETA: 8s - loss: 0.658 - ETA: 8s - loss: 0.657 - ETA: 8s - loss: 0.656 - ETA: 8s - loss: 0.654 - ETA: 8s - loss: 0.653 - ETA: 8s - loss: 0.653 - ETA: 8s - loss: 0.652 - ETA: 8s - loss: 0.652 - ETA: 7s - loss: 0.652 - ETA: 7s - loss: 0.652 - ETA: 7s - loss: 0.652 - ETA: 7s - loss: 0.650 - ETA: 7s - loss: 0.651 - ETA: 7s - loss: 0.650 - ETA: 7s - loss: 0.649 - ETA: 7s - loss: 0.649 - ETA: 7s - loss: 0.648 - ETA: 6s - loss: 0.646 - ETA: 6s - loss: 0.646 - ETA: 6s - loss: 0.646 - ETA: 6s - loss: 0.645 - ETA: 6s - loss: 0.644 - ETA: 6s - loss: 0.643 - ETA: 6s - loss: 0.642 - ETA: 6s - loss: 0.641 - ETA: 6s - loss: 0.639 - ETA: 6s - loss: 0.639 - ETA: 5s - loss: 0.638 - ETA: 5s - loss: 0.637 - ETA: 5s - loss: 0.636 - ETA: 5s - loss: 0.637 - ETA: 5s - loss: 0.636 - ETA: 5s - loss: 0.635 - ETA: 5s - loss: 0.635 - ETA: 5s - loss: 0.634 - ETA: 5s - loss: 0.634 - ETA: 4s - loss: 0.633 - ETA: 4s - loss: 0.632 - ETA: 4s - loss: 0.632 - ETA: 4s - loss: 0.631 - ETA: 4s - loss: 0.630 - ETA: 4s - loss: 0.629 - ETA: 4s - loss: 0.629 - ETA: 4s - loss: 0.629 - ETA: 4s - loss: 0.628 - ETA: 4s - loss: 0.627 - ETA: 3s - loss: 0.626 - ETA: 3s - loss: 0.626 - ETA: 3s - loss: 0.625 - ETA: 3s - loss: 0.624 - ETA: 3s - loss: 0.624 - ETA: 3s - loss: 0.623 - ETA: 3s - loss: 0.622 - ETA: 3s - loss: 0.622 - ETA: 3s - loss: 0.622 - ETA: 3s - loss: 0.621 - ETA: 2s - loss: 0.620 - ETA: 2s - loss: 0.620 - ETA: 2s - loss: 0.620 - ETA: 2s - loss: 0.619 - ETA: 2s - loss: 0.619 - ETA: 2s - loss: 0.619 - ETA: 2s - loss: 0.618 - ETA: 2s - loss: 0.618 - ETA: 2s - loss: 0.616 - ETA: 2s - loss: 0.616 - ETA: 1s - loss: 0.615 - ETA: 1s - loss: 0.614 - ETA: 1s - loss: 0.614 - ETA: 1s - loss: 0.613 - ETA: 1s - loss: 0.613 - ETA: 1s - loss: 0.612 - ETA: 1s - loss: 0.612 - ETA: 1s - loss: 0.611 - ETA: 1s - loss: 0.610 - ETA: 1s - loss: 0.609 - ETA: 1s - loss: 0.609 - ETA: 0s - loss: 0.609 - ETA: 0s - loss: 0.608 - ETA: 0s - loss: 0.608 - ETA: 0s - loss: 0.608 - ETA: 0s - loss: 0.607 - ETA: 0s - loss: 0.607 - ETA: 0s - loss: 0.608 - ETA: 0s - loss: 0.608 - ETA: 0s - loss: 0.608 - ETA: 0s - loss: 0.607 - 23s 3ms/step - loss: 0.6071\n",
      "RNN-LSTM, Word Embeddings 0.5268\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_lstm():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "    \n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
    "    \n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "    \n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model \n",
    "\n",
    "classifier = create_rnn_lstm()\n",
    "accuracy = train_model(classifier, train_seq_x, valid_seq_x, is_neural_net=True)\n",
    "print(\"RNN-LSTM, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recurrent Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - ETA: 14:02 - loss: 0.68 - ETA: 7:51 - loss: 0.6960 - ETA: 5:47 - loss: 0.692 - ETA: 4:45 - loss: 0.706 - ETA: 4:06 - loss: 0.702 - ETA: 3:40 - loss: 0.700 - ETA: 3:22 - loss: 0.698 - ETA: 3:09 - loss: 0.695 - ETA: 2:58 - loss: 0.694 - ETA: 2:49 - loss: 0.693 - ETA: 2:41 - loss: 0.691 - ETA: 2:34 - loss: 0.692 - ETA: 2:28 - loss: 0.692 - ETA: 2:23 - loss: 0.692 - ETA: 2:19 - loss: 0.692 - ETA: 2:15 - loss: 0.695 - ETA: 2:12 - loss: 0.694 - ETA: 2:08 - loss: 0.693 - ETA: 2:06 - loss: 0.694 - ETA: 2:03 - loss: 0.693 - ETA: 2:01 - loss: 0.693 - ETA: 1:58 - loss: 0.693 - ETA: 1:56 - loss: 0.691 - ETA: 1:54 - loss: 0.691 - ETA: 1:52 - loss: 0.691 - ETA: 1:50 - loss: 0.690 - ETA: 1:49 - loss: 0.688 - ETA: 1:47 - loss: 0.687 - ETA: 1:46 - loss: 0.687 - ETA: 1:44 - loss: 0.685 - ETA: 1:43 - loss: 0.685 - ETA: 1:42 - loss: 0.685 - ETA: 1:40 - loss: 0.685 - ETA: 1:39 - loss: 0.684 - ETA: 1:38 - loss: 0.683 - ETA: 1:37 - loss: 0.681 - ETA: 1:36 - loss: 0.682 - ETA: 1:35 - loss: 0.682 - ETA: 1:34 - loss: 0.682 - ETA: 1:33 - loss: 0.681 - ETA: 1:32 - loss: 0.681 - ETA: 1:31 - loss: 0.680 - ETA: 1:30 - loss: 0.679 - ETA: 1:29 - loss: 0.679 - ETA: 1:29 - loss: 0.679 - ETA: 1:28 - loss: 0.679 - ETA: 1:27 - loss: 0.679 - ETA: 1:26 - loss: 0.676 - ETA: 1:25 - loss: 0.674 - ETA: 1:25 - loss: 0.672 - ETA: 1:24 - loss: 0.671 - ETA: 1:23 - loss: 0.670 - ETA: 1:22 - loss: 0.670 - ETA: 1:22 - loss: 0.667 - ETA: 1:21 - loss: 0.665 - ETA: 1:20 - loss: 0.664 - ETA: 1:20 - loss: 0.662 - ETA: 1:19 - loss: 0.661 - ETA: 1:18 - loss: 0.659 - ETA: 1:18 - loss: 0.659 - ETA: 1:17 - loss: 0.655 - ETA: 1:17 - loss: 0.655 - ETA: 1:16 - loss: 0.653 - ETA: 1:15 - loss: 0.650 - ETA: 1:15 - loss: 0.649 - ETA: 1:14 - loss: 0.645 - ETA: 1:14 - loss: 0.644 - ETA: 1:13 - loss: 0.642 - ETA: 1:13 - loss: 0.639 - ETA: 1:12 - loss: 0.636 - ETA: 1:11 - loss: 0.635 - ETA: 1:11 - loss: 0.634 - ETA: 1:10 - loss: 0.631 - ETA: 1:10 - loss: 0.630 - ETA: 1:09 - loss: 0.626 - ETA: 1:09 - loss: 0.623 - ETA: 1:08 - loss: 0.620 - ETA: 1:08 - loss: 0.617 - ETA: 1:07 - loss: 0.614 - ETA: 1:06 - loss: 0.614 - ETA: 1:06 - loss: 0.612 - ETA: 1:05 - loss: 0.609 - ETA: 1:05 - loss: 0.607 - ETA: 1:04 - loss: 0.605 - ETA: 1:04 - loss: 0.604 - ETA: 1:03 - loss: 0.606 - ETA: 1:03 - loss: 0.604 - ETA: 1:02 - loss: 0.602 - ETA: 1:02 - loss: 0.603 - ETA: 1:01 - loss: 0.604 - ETA: 1:01 - loss: 0.603 - ETA: 1:00 - loss: 0.602 - ETA: 1:00 - loss: 0.600 - ETA: 59s - loss: 0.597 - ETA: 59s - loss: 0.59 - ETA: 59s - loss: 0.59 - ETA: 58s - loss: 0.59 - ETA: 58s - loss: 0.59 - ETA: 57s - loss: 0.59 - ETA: 57s - loss: 0.58 - ETA: 56s - loss: 0.58 - ETA: 56s - loss: 0.58 - ETA: 55s - loss: 0.58 - ETA: 55s - loss: 0.58 - ETA: 54s - loss: 0.58 - ETA: 54s - loss: 0.58 - ETA: 53s - loss: 0.58 - ETA: 53s - loss: 0.58 - ETA: 52s - loss: 0.57 - ETA: 52s - loss: 0.57 - ETA: 51s - loss: 0.57 - ETA: 51s - loss: 0.57 - ETA: 51s - loss: 0.57 - ETA: 50s - loss: 0.56 - ETA: 50s - loss: 0.56 - ETA: 49s - loss: 0.56 - ETA: 49s - loss: 0.56 - ETA: 48s - loss: 0.55 - ETA: 48s - loss: 0.55 - ETA: 47s - loss: 0.55 - ETA: 47s - loss: 0.55 - ETA: 46s - loss: 0.55 - ETA: 46s - loss: 0.55 - ETA: 46s - loss: 0.55 - ETA: 45s - loss: 0.55 - ETA: 45s - loss: 0.54 - ETA: 44s - loss: 0.54 - ETA: 44s - loss: 0.54 - ETA: 43s - loss: 0.54 - ETA: 43s - loss: 0.54 - ETA: 43s - loss: 0.54 - ETA: 42s - loss: 0.54 - ETA: 42s - loss: 0.54 - ETA: 41s - loss: 0.54 - ETA: 41s - loss: 0.54 - ETA: 40s - loss: 0.54 - ETA: 40s - loss: 0.53 - ETA: 39s - loss: 0.53 - ETA: 39s - loss: 0.53 - ETA: 39s - loss: 0.53 - ETA: 38s - loss: 0.53 - ETA: 38s - loss: 0.53 - ETA: 37s - loss: 0.53 - ETA: 37s - loss: 0.53 - ETA: 36s - loss: 0.53 - ETA: 36s - loss: 0.53 - ETA: 36s - loss: 0.53 - ETA: 35s - loss: 0.53 - ETA: 35s - loss: 0.53 - ETA: 34s - loss: 0.52 - ETA: 34s - loss: 0.52 - ETA: 33s - loss: 0.52 - ETA: 33s - loss: 0.52 - ETA: 33s - loss: 0.52 - ETA: 32s - loss: 0.52 - ETA: 32s - loss: 0.52 - ETA: 31s - loss: 0.52 - ETA: 31s - loss: 0.52 - ETA: 30s - loss: 0.52 - ETA: 30s - loss: 0.52 - ETA: 30s - loss: 0.52 - ETA: 29s - loss: 0.51 - ETA: 29s - loss: 0.51 - ETA: 28s - loss: 0.51 - ETA: 28s - loss: 0.51 - ETA: 28s - loss: 0.51 - ETA: 27s - loss: 0.51 - ETA: 27s - loss: 0.51 - ETA: 26s - loss: 0.51 - ETA: 26s - loss: 0.51 - ETA: 25s - loss: 0.51 - ETA: 25s - loss: 0.51 - ETA: 25s - loss: 0.51 - ETA: 24s - loss: 0.51 - ETA: 24s - loss: 0.50 - ETA: 23s - loss: 0.50 - ETA: 23s - loss: 0.50 - ETA: 23s - loss: 0.50 - ETA: 22s - loss: 0.50 - ETA: 22s - loss: 0.50 - ETA: 21s - loss: 0.50 - ETA: 21s - loss: 0.50 - ETA: 20s - loss: 0.49 - ETA: 20s - loss: 0.50 - ETA: 20s - loss: 0.49 - ETA: 19s - loss: 0.49 - ETA: 19s - loss: 0.49 - ETA: 18s - loss: 0.49 - ETA: 18s - loss: 0.49 - ETA: 18s - loss: 0.49 - ETA: 17s - loss: 0.49 - ETA: 17s - loss: 0.49 - ETA: 16s - loss: 0.49 - ETA: 16s - loss: 0.49 - ETA: 16s - loss: 0.49 - ETA: 15s - loss: 0.49 - ETA: 15s - loss: 0.48 - ETA: 14s - loss: 0.48 - ETA: 14s - loss: 0.48 - ETA: 14s - loss: 0.48 - ETA: 13s - loss: 0.48 - ETA: 13s - loss: 0.48 - ETA: 12s - loss: 0.48 - ETA: 12s - loss: 0.48 - ETA: 11s - loss: 0.48 - ETA: 11s - loss: 0.48 - ETA: 11s - loss: 0.48 - ETA: 10s - loss: 0.48 - ETA: 10s - loss: 0.48 - ETA: 9s - loss: 0.4823 - ETA: 9s - loss: 0.481 - ETA: 9s - loss: 0.481 - ETA: 8s - loss: 0.480 - ETA: 8s - loss: 0.479 - ETA: 7s - loss: 0.478 - ETA: 7s - loss: 0.478 - ETA: 7s - loss: 0.479 - ETA: 6s - loss: 0.479 - ETA: 6s - loss: 0.478 - ETA: 5s - loss: 0.477 - ETA: 5s - loss: 0.477 - ETA: 5s - loss: 0.476 - ETA: 4s - loss: 0.475 - ETA: 4s - loss: 0.475 - ETA: 3s - loss: 0.475 - ETA: 3s - loss: 0.475 - ETA: 2s - loss: 0.474 - ETA: 2s - loss: 0.473 - ETA: 2s - loss: 0.473 - ETA: 1s - loss: 0.473 - ETA: 1s - loss: 0.472 - ETA: 0s - loss: 0.471 - ETA: 0s - loss: 0.471 - ETA: 0s - loss: 0.470 - 95s 13ms/step - loss: 0.4700\n",
      "RCNN, Word Embeddings 0.5268\n"
     ]
    }
   ],
   "source": [
    "def create_rcnn():\n",
    "    input_layer = layers.Input((70,))\n",
    "    \n",
    "    embedding_layer = layers.Embedding(len(word_index)+1, 300, weights=[embedding_matrix], trainable=True)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    recurrent_layer = layers.Bidirectional(layers.LSTM(100, return_sequences=True))(embedding_layer)\n",
    "    \n",
    "    conv_layer = layers.Convolution1D(100,3,activation='relu')(recurrent_layer)\n",
    "    \n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "    \n",
    "    output_layer1 = layers.Dense(50, activation='relu')(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation = 'sigmoid')(output_layer1)\n",
    "    \n",
    "    model = models.Model(inputs = input_layer, outputs = output_layer2)\n",
    "    model.compile(optimizer = optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rcnn()\n",
    "accuracy = train_model(classifier, train_seq_x, valid_seq_x, is_neural_net=True)\n",
    "print(\"RCNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
